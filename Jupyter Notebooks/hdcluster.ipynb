{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.cluster import MeanShift\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import debacl as dcl\n",
    "from datetime import datetime, timedelta\n",
    "X=np.array([[1.5,2,3],[2.5,5,8],[5.5,10,11]])\n",
    "Y=np.array([[2,3,4],[2.5,3.5,4.5],[5.5,6,7],[6,7,8]])\n",
    "a=distance_matrix(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the file path for loading, data file is under the same dir with the notebook\n",
    "filename=\"20121015_00_ecmwf_ensemble_forecast.PRESSURE_LEVELS.EUR_LL10.120.pl.nc\"\n",
    "foldername=\"ECWMF Datasets\"\n",
    "filepath=os.path.join(os.path.dirname(os.getcwd()),foldername,filename)\n",
    "\n",
    "# read the raw data and extract the needed data\n",
    "# exrtact the value of Geopotential under the pressure of 500 hPA in the certain\n",
    "Pressure_Levels_data = nc.Dataset(filepath,\"r\")\n",
    "g = 9.80655\n",
    "# get all the dimension value\n",
    "nd_1,nd_2,nd_3,nd_4,nd_5 = Pressure_Levels_data.variables['Geopotential_isobaric'][:].shape\n",
    "# get the necessary raw data\n",
    "Geopotential_Isobaric_500 = Pressure_Levels_data.variables['Geopotential_isobaric'][0,:,7,:,:]/g\n",
    "# reshape the dataset into form of (51,41*101)\n",
    "Geopotential_Isobaric_500_reshaped = np.reshape(Geopotential_Isobaric_500,(nd_2, nd_4 * nd_5))\n",
    "# prepare the longitude and latitude value for contour\n",
    "longitude = Pressure_Levels_data['lon'][:]\n",
    "latitude = Pressure_Levels_data['lat'][:]\n",
    "(lon, lat) = np.meshgrid(longitude, latitude)\n",
    "\n",
    "# use PCA to reduce dimensions under the condition of reaching 80% of all the member infomation\n",
    "exp_var = 0\n",
    "n_pc = 0\n",
    "while exp_var < 0.8:\n",
    "    n_pc = n_pc + 1\n",
    "    pca = PCA(n_components = n_pc)\n",
    "    pca.fit(Geopotential_Isobaric_500_reshaped)\n",
    "    exp_var = sum(pca.explained_variance_ratio_)\n",
    "\n",
    "# get the transformed raw data in the dimension-reduced space    \n",
    "pca_transformed_data = pca.transform(Geopotential_Isobaric_500_reshaped)\n",
    "\n",
    "#get the time variable\n",
    "times=Pressure_Levels_data.variables[\"time\"]\n",
    "#get the time number\n",
    "arrDateEnd=nc.num2date(times[:],units=times.units)\n",
    "#get the time in date format\n",
    "dateEndDate = datetime.date(arrDateEnd[0]).strftime(\"%d %b %Y\")\n",
    "dateEndMin = datetime.date(arrDateEnd[0]).strftime(\"%H:%M\")\n",
    "dateStart=datetime.date(arrDateEnd[0])-timedelta(hours=120)\n",
    "dateStartDate=dateStart.strftime(\"%d %b %Y\")\n",
    "dateStartMin=dateStart.strftime(\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Subgrid(longitude,latitude,bottomLeft,topRight):\n",
    "    lonStart=(np.abs(longitude - bottomLeft[0])).argmin()\n",
    "    lonEnd=(np.abs(longitude - topRight[0])).argmin()\n",
    "    latStart=(np.abs(latitude - bottomLeft[1])).argmin()\n",
    "    latEnd=(np.abs(latitude - topRight[1])).argmin()\n",
    "    \n",
    "    return lonStart,lonEnd,latStart,latEnd\n",
    "\n",
    "def Get_Meshgrid(lonStart,lonEnd,latStart,latEnd):\n",
    "    long=longitude[lonStart:lonEnd+1]\n",
    "    lati=latitude[latEnd:latStart+1]\n",
    "    (lon,lat)=np.meshgrid(long,lati)\n",
    "    \n",
    "    return lon,lat\n",
    "\n",
    "def Get_Area_Data(data,lonStart,lonEnd,latStart,latEnd):\n",
    "    areaData=data[:,latEnd:latStart+1,lonStart:lonEnd+1]\n",
    "    \n",
    "    return areaData\n",
    "\n",
    "def Reshape_New_Data(areaData):\n",
    "    dim1,dim2,dim3=areaData.shape\n",
    "    reshapedData=np.reshape(areaData,(dim1,dim2*dim3))\n",
    "    return reshapedData\n",
    "\n",
    "def PCA_Run(data):\n",
    "    exp_var = 0\n",
    "    n_pc = 0\n",
    "    while exp_var < 0.8:\n",
    "        n_pc = n_pc + 1\n",
    "        pca = PCA(n_components = n_pc)\n",
    "        pca.fit(data)\n",
    "        exp_var = sum(pca.explained_variance_ratio_)\n",
    "        \n",
    "    return pca\n",
    "\n",
    "def decompose_region(data,lon,lat,point1,point2):\n",
    "    lonStart,lonEnd,latStart,latEnd = Get_Subgrid(lon,lat,point1,point2)\n",
    "    secData = Get_Area_Data(data,lonStart,lonEnd,latStart,latEnd)\n",
    "    reshapeData=Reshape_New_Data(secData)\n",
    "    PCA_=PCA_Run(reshapeData)\n",
    "    transformedData=PCA_.transform(reshapeData)\n",
    "    \n",
    "    return transformedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#get the file path for loading, data file is under the same dir with the notebook\n",
    "filename=\"20121017_12_ecmwf_forecast.PRESSURE_LEVELS.EUR_LL015.036.pl.nc\"\n",
    "foldername=\"ECWMF Datasets\"\n",
    "filepath=os.path.join(os.path.dirname(os.getcwd()),foldername,filename)\n",
    "syn = nc.Dataset(filepath,\"r\")\n",
    "#syn.variables\n",
    "longitude1 = syn['lon'][:]\n",
    "latitude1 = syn['lat'][:]\n",
    "(lon1, lat1) = np.meshgrid(longitude1, latitude1)\n",
    "syndata =syn.variables['Geopotential_isobaric'][:]/9.8\n",
    "syndataiso1 = syndata[0,:,0,:,:]\n",
    "syndataiso2 = syndata[0,:,1,:,:]\n",
    "syndataiso3 = syndata[0,:,2,:,:]\n",
    "\n",
    "synsmall=decompose_region(syndataiso1,longitude1,latitude1,[-10,35],[20,65])\n",
    "\n",
    "realsmall=decompose_region(Geopotential_Isobaric_500,longitude,latitude,[-60,45],[-40,60])\n",
    "realmid=decompose_region(Geopotential_Isobaric_500,longitude,latitude,[-60,30],[10,60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandwidth_selection(data,sigThresh,outlierThresh):\n",
    "    #constant\n",
    "    step = 1\n",
    "    n_clus = 0\n",
    "\n",
    "    D = distance_matrix(X,X)\n",
    "    minD = int(np.round(np.min(D[np.nonzero(D)])))\n",
    "    if minD<0:\n",
    "        minD=2\n",
    "    maxD = int(np.round(np.max(D[np.nonzero(D)])))\n",
    "\n",
    "    outlierVec=[]\n",
    "    hCandidate=[]\n",
    "\n",
    "    for i_h in range(int(np.round(minD/2)),maxD+1,step):\n",
    "        meanShift=MeanShift(bandwidth=i_h).fit(X)\n",
    "        labels=meanShift.labels_\n",
    "\n",
    "        cl_sig=[k for k, v in Counter(labels).items() if v>=sigThresh]\n",
    "        \n",
    "        n_sig_new = len(cl_sig)\n",
    "        numOutlierModes = len(set(labels))-len(cl_sig)\n",
    "        \n",
    "        if n_sig_new>n_clus:\n",
    "            n_clus=n_sig_new\n",
    "            hCandidate.append(i_h)\n",
    "            outlierVec.append(numOutlierModes)\n",
    "        elif n_sig_new==n_clus:\n",
    "            hCandidate.append(i_h)\n",
    "            outlierVec.append(numOutlierModes)\n",
    "    \n",
    "    h=hCandidate[-1]\n",
    "    for i in range(len(hCandidate)):\n",
    "        if outlierVec[i]<=outlierThresh:\n",
    "            h=hCandidate[i]\n",
    "            break\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(data,modes):\n",
    "    modClosestMemID=np.zeros(len(modes))\n",
    "    dist=np.zeros(len(modes))\n",
    "    for idx,mode in enumerate(modes):\n",
    "        if len(mode.shape)==1:\n",
    "            mode=mode[np.newaxis,:]\n",
    "        modeKNNid=np.argmin(euclidean_distances(data,mode))\n",
    "        minDist=np.min(euclidean_distances(data,mode))\n",
    "        modClosestMemID[idx]=modeKNNid\n",
    "        dist[idx]=minDist\n",
    "        \n",
    "    return modClosestMemID,dist\n",
    "\n",
    "def pairConn(X1,X2,lv,data,h):\n",
    "    IDX,D=knn_search(X1,X2)\n",
    "    #idx_2=np.where(D==D.min())\n",
    "    #idx_1=IDX[idx_2]\n",
    "    #minDist=D.min()\n",
    "    meanDist=np.mean(D)\n",
    "    \n",
    "    isConn=1\n",
    "    for one in range(len(X1)):\n",
    "        for two in range(len(X2)):\n",
    "            q1=X1[one,:]\n",
    "            q2=X2[two,:]\n",
    "            \n",
    "            n_seq=20\n",
    "            nl=np.zeros((n_seq,data.shape[1]))\n",
    "            for i in range(data.shape[1]):\n",
    "                s=q1[i]\n",
    "                e=q2[i]\n",
    "                if s==e:\n",
    "                    nl[:,i]=np.zeros((n_seq,1))+s\n",
    "                else:\n",
    "                    nl[:,i]=np.linspace(s,e,n_seq)\n",
    "            \n",
    "            kde=KernelDensity(kernel=\"gaussian\",bandwidth=h).fit(data)\n",
    "            f_nl=np.exp(kde.score_samples(nl))\n",
    "            \n",
    "            if (np.sum(f_nl[f_nl<lv])>0) or (meanDist>2*h):\n",
    "                isConn=0\n",
    "                break\n",
    "    return isConn\n",
    "\n",
    "def cluConn(data,f,labels,lv,h):\n",
    "    n_clu=len(set(labels))\n",
    "    connMat=np.zeros((n_clu,n_clu))\n",
    "    \n",
    "    for i in range(n_clu-1):\n",
    "        for j in range(i+1,n_clu):\n",
    "            id_i=np.where(labels==i)\n",
    "            id_j=np.where(labels==j)\n",
    "            \n",
    "            connMat[i,j]=pairConn(data[id_i],data[id_j],lv,data,h)\n",
    "            connMat[j,i]=connMat[i,j]\n",
    "            \n",
    "    return connMat \n",
    "\n",
    "def high_density_clustering(data,h,sizeThresh):\n",
    "    meanShift=MeanShift(bandwidth=h).fit(data)\n",
    "    labels=meanShift.labels_\n",
    "    modes=meanShift.cluster_centers_\n",
    "    #modClosestMemID=knn_search(data,modes)\n",
    "    n_clu=len(set(labels))\n",
    "    \n",
    "    kde=KernelDensity(kernel=\"gaussian\",bandwidth=h).fit(data)\n",
    "    f=np.exp(kde.score_samples(data))\n",
    "    \n",
    "    n_lv=20\n",
    "    lv_seq=[i/n_lv*f.max() for i in range(1,n_lv+1)]\n",
    "    \n",
    "    #check the cluster info at each level: member, size and connectivity\n",
    "    conn=np.empty((1,n_lv),dtype=object)\n",
    "    l_conn=np.empty((1,n_lv),dtype=object)\n",
    "    for i in range(n_lv):\n",
    "        conn[0,i]=cluConn(data,f,labels,lv_seq[i],h)\n",
    "    \n",
    "    #check which clusters are connected at each level\n",
    "    conn_mask=np.zeros(conn[0,0].shape)\n",
    "    for i in reversed(range(n_lv)):\n",
    "        lvl_conn=np.zeros(conn[0,i].shape)\n",
    "        lvl_tmp=lvl_conn\n",
    "        tmp=lvl_tmp\n",
    "        for j in range(i,n_lv):\n",
    "            tmp=tmp+conn[0,j]\n",
    "        lvl_tmp=np.where(tmp!=1,lvl_tmp,1)\n",
    "        \n",
    "        #fing indices of newly connected clusters\n",
    "        r_c,c_c=np.where(lvl_tmp==1)\n",
    "        \n",
    "        #for any pair of connected clusters, check if they are already connected\n",
    "        for k in range(len(r_c)):\n",
    "            if conn_mask[r_c[k],c_c[k]]==0:\n",
    "                neighors1=np.where(conn_mask[r_c[k],:]==1)\n",
    "                neighors2=np.where(conn_mask[c_c[k],:]==1)\n",
    "                \n",
    "                conn_mask[r_c[k],c_c[k]]=1\n",
    "                conn_mask[c_c[k],r_c[k]]=1\n",
    "                \n",
    "                conn_mask[neighors1,c_c[k]]=1\n",
    "                conn_mask[c_c[k],neighors1]=1\n",
    "                conn_mask[neighors2,r_c[k]]=1\n",
    "                conn_mask[r_c[k],neighors2]=1\n",
    "                \n",
    "                lvl_conn[r_c[k],c_c[k]]=1\n",
    "                lvl_conn[c_c[k],r_c[k]]=1\n",
    "        \n",
    "        l_conn[0,i]=lvl_conn\n",
    "        \n",
    "    #cluster members by density\n",
    "    upperLvlMembers=np.empty((n_lv,n_clu),dtype=object)\n",
    "    lvlMembers=np.empty((n_lv,n_clu),dtype=object)\n",
    "    upper_size=np.zeros((n_lv,n_clu))\n",
    "      \n",
    "    for i in range(n_lv-1):\n",
    "        upper_idx=np.where(f>lv_seq[i])\n",
    "        lvl_idx=np.where((f>lv_seq[i]) & (f<=lv_seq[i+1]))\n",
    "        for j in range(n_clu):\n",
    "            upperLvlMembers[i,j]=np.intersect1d(upper_idx,np.where(labels==j))\n",
    "            lvlMembers[i,j]=np.intersect1d(lvl_idx,np.where(labels==j))\n",
    "            upper_size[i,j]=len(upperLvlMembers[i,j])\n",
    "            \n",
    "    #select significant clusters just in case\n",
    "    sig_clu=[k for k, v in Counter(labels).items() if v>=sizeThresh]\n",
    "            \n",
    "    return upperLvlMembers,lvlMembers,upper_size,sig_clu,f,conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandwidth_selection(synsmall,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandwidth_selection(realsmall,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandwidth_selection(realmid,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
